import tensorflow as tf
# ==========================================================
# üîπ PR√ÅCTICA IA - CLASIFICACI√ìN CON CNN (Fashion-MNIST)
# ==========================================================
# Este Notebook entrena una red neuronal convolucional (CNN)
# para clasificar im√°genes del dataset Fashion-MNIST.
# ----------------------------------------------------------

# --- Importaci√≥n de librer√≠as necesarias ---
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical

# ==========================================================
# üß© 1. Cargar y Pre-procesar los Datos (Fashion-MNIST)
# ==========================================================

# Cargamos el dataset predividido en entrenamiento y prueba
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

# Normalizamos los valores de p√≠xeles (0 a 255 ‚Üí 0.0 a 1.0)
# Esto mejora la estabilidad num√©rica y acelera el aprendizaje
X_train_full = X_train_full / 255.0
X_test = X_test / 255.0

# Reformateamos las im√°genes a 4D (batch, alto, ancho, canales)
# CNN espera im√°genes con 1 canal (escala de grises)
X_train = X_train_full.reshape((60000, 28, 28, 1))
X_test = X_test.reshape((10000, 28, 28, 1))

# Convertimos las etiquetas a formato categ√≥rico (one-hot)
# Ejemplo: 3 ‚Üí [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
y_train = to_categorical(y_train_full, 10)
y_test_cat = to_categorical(y_test, 10)

# Mostramos tama√±os para verificar consistencia
print("Tama√±o del set de entrenamiento:", X_train.shape)
print("Tama√±o del set de prueba:", X_test.shape)

# ==========================================================
# üß† 2. Definir la Arquitectura del Modelo CNN
# ==========================================================

# Creamos un modelo secuencial (capa por capa)
model = Sequential()

# --- Etapa de Extracci√≥n de Caracter√≠sticas ---
# Primera capa convolucional: 32 filtros 3x3 con activaci√≥n ReLU
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))

# Pooling 2x2: reduce dimensiones, mantiene rasgos importantes
model.add(MaxPooling2D((2, 2)))

# Segunda capa convolucional: 64 filtros 3x3
model.add(Conv2D(64, (3, 3), activation='relu'))

# Segundo pooling para reducir tama√±o de nuevo
model.add(MaxPooling2D((2, 2)))

# --- Etapa de Clasificaci√≥n ---
# Aplanamos las caracter√≠sticas 2D a vector 1D
model.add(Flatten())

# Capa densa oculta de 128 neuronas con ReLU
model.add(Dense(128, activation='relu'))

# Capa de salida: 10 neuronas (una por clase), activaci√≥n softmax
model.add(Dense(10, activation='softmax'))

# Resumen del modelo (√∫til para Notebook)
print("üìò Arquitectura del modelo CNN:")
model.summary()

# ==========================================================
# ‚öôÔ∏è 3. Compilar el Modelo
# ==========================================================
# Definimos el optimizador, la funci√≥n de p√©rdida y las m√©tricas
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# ==========================================================
# üöÄ 4. Entrenar el Modelo
# ==========================================================
print("\n--- Iniciando Entrenamiento ---")

# Entrenamos la red:
# - epochs: n√∫mero de pasadas por todo el dataset
# - batch_size: tama√±o de los lotes de entrenamiento
# - validation_split: 10% de los datos para validaci√≥n
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=64,
    validation_split=0.1
)

print("--- Entrenamiento Finalizado ---")

# ==========================================================
# üìä 5. Evaluar y Registrar M√©tricas
# ==========================================================
print("\n--- Evaluaci√≥n en el set de Prueba ---")

# Evaluamos en los datos de prueba no vistos
loss, acc = model.evaluate(X_test, y_test_cat)

# Mostramos resultados finales
print(f"P√©rdida (Loss) en Test: {loss:.4f}")
print(f"Precisi√≥n (Accuracy) en Test: {acc*100:.2f}%")

# ==========================================================
# üìà (Opcional) Visualizar el historial de entrenamiento
# ==========================================================
# En una celda extra en Jupyter pod√©s graficar la evoluci√≥n:
#
# import matplotlib.pyplot as plt
# plt.plot(history.history['accuracy'], label='Entrenamiento')
# plt.plot(history.history['val_accuracy'], label='Validaci√≥n')
# plt.title('Precisi√≥n a lo largo de las √©pocas')
# plt.xlabel('√âpoca')
# plt.ylabel('Precisi√≥n')
# plt.legend()
# plt.show()
#
# Esto te permite visualizar el rendimiento del modelo.
# ==========================================================
