# ============================================
# 游닂 REGRESI칍N LINEAL VS POLINOMIAL EN PYTHON
# ============================================

# En este notebook se entrena un modelo para predecir el puntaje de matem치ticas ("math score")
# de estudiantes, usando las dem치s variables del dataset "StudentsPerformance.csv".
# Luego se compara el rendimiento entre un modelo lineal y uno polinomial.

# Importaci칩n de librer칤as necesarias
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# ============================================
# 游댳 CARGA DEL DATASET
# ============================================

# Leemos el archivo CSV con la informaci칩n de los estudiantes
data = pd.read_csv("StudentsPerformance.csv")

# ============================================
# 游댳 DEFINICI칍N DE VARIABLES
# ============================================

# Variable objetivo (target): "math score"
# Variables predictoras (features): todas las dem치s columnas
X = data.drop("math score", axis=1)
y = data["math score"]

# ============================================
# 游댳 DIVISI칍N ENTRE TRAIN Y TEST
# ============================================

# Dividimos los datos en 80% entrenamiento y 20% prueba
# random_state=42 asegura reproducibilidad de resultados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ============================================
# 游댳 AN츼LISIS OPCIONAL
# ============================================

# Si se desea visualizar las distribuciones de las variables
# descomentar las siguientes l칤neas:
# data.hist(bins=50, figsize=(12, 8))
# plt.show()

# ============================================
# 游댳 IDENTIFICACI칍N DE VARIABLES CATEG칍RICAS Y NUM칄RICAS
# ============================================

# Detectamos las variables categ칩ricas (tipo texto)
cat_features = X.select_dtypes(include="object").columns

# Detectamos las variables num칠ricas (si las hubiera)
num_features = X.select_dtypes(exclude="object").columns

# ============================================
# 游댳 MODELO POLINOMIAL (Grado 3)
# ============================================

# Creamos las caracter칤sticas polin칩micas sobre las variables num칠ricas
# Esto permite capturar relaciones no lineales
poly = PolynomialFeatures(degree=3, include_bias=False)

# Definimos el preprocesamiento:
# - Codificamos las variables categ칩ricas con OneHotEncoder
# - Estandarizamos las variables num칠ricas y aplicamos expansi칩n polin칩mica
preprocessor_poly = ColumnTransformer([
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features),
    ("num", Pipeline([("scaler", StandardScaler()), ("poly", poly)]), num_features)
])

# Creamos el pipeline completo:
# 1. Preprocesamiento de datos
# 2. Regresi칩n lineal sobre las features transformadas
poly_model = Pipeline([
    ("preprocessor", preprocessor_poly),
    ("regressor", LinearRegression())
])

# Entrenamos el modelo polinomial con el conjunto de entrenamiento
poly_model.fit(X_train, y_train)

# ============================================
# 游댳 EVALUACI칍N DEL MODELO POLINOMIAL
# ============================================

# Realizamos predicciones sobre el conjunto de prueba
y_pred_poly = poly_model.predict(X_test)

# ============================================
# 游댳 MODELO LINEAL BASE
# ============================================

# Importamos nuevamente Pipeline (ya est치 arriba, pero se deja por claridad)
from sklearn.pipeline import Pipeline

# Definimos un modelo lineal b치sico sin t칠rminos polin칩micos
lin_model = Pipeline([
    ("preprocessor", ColumnTransformer([
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features),
        ("num", StandardScaler(), num_features)
    ])),
    ("regressor", LinearRegression())
])

# Entrenamos el modelo lineal
lin_model.fit(X_train, y_train)

# Realizamos predicciones con el modelo lineal
y_pred_lin = lin_model.predict(X_test)

# ============================================
# 游댳 COMPARACI칍N DE M칄TRICAS
# ============================================

# Calculamos el Error Cuadr치tico Medio (MSE) y el Coeficiente de Determinaci칩n (R)
# para ambos modelos y comparamos sus resultados
print("MSE (Lineal):", mean_squared_error(y_test, y_pred_lin))
print("R (Lineal):", r2_score(y_test, y_pred_lin))

print("MSE (Polinomial grado 2):", mean_squared_error(y_test, y_pred_poly))
print("R (Polinomial grado 2):", r2_score(y_test, y_pred_poly))

# ============================================
# 游댳 VISUALIZACI칍N DE RESULTADOS
# ============================================

# Graficamos las predicciones frente a los valores reales para ambos modelos
plt.scatter(y_test, y_pred_lin, alpha=0.6, color="blue", label="Lineal")              # Puntos azules: modelo lineal
plt.scatter(y_test, y_pred_poly, alpha=0.6, color="red", label="Polinomial grado 2")  # Puntos rojos: modelo polinomial
plt.xlabel("Valores reales (math score)")    # Eje X: puntaje verdadero
plt.ylabel("Predicciones")                   # Eje Y: puntaje predicho
plt.title("Comparaci칩n Lineal vs Polinomial") # T칤tulo del gr치fico
plt.legend()                                 # Mostrar leyenda
plt.show()                                   # Mostrar la figura
